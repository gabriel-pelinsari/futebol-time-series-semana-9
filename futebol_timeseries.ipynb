{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcc5ac7a-df44-4731-9b31-a6c6c3b958f2",
   "metadata": {},
   "source": [
    "# Futebol Time Series Prediction\n",
    "\n",
    "Este notebook desenvolve um modelo de predição para uma série temporal com tema de futebol.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Como referência, consultamos um conjunto de dados de futebol no Kaggle que compila resultados de jogos internacionais desde 1872 até 2025 (\"International football results from 1872 to 2025\")【179248037889072†L21-L55】.  \n",
    "Para simplificar a tarefa e evitar dependências externas, construímos um subconjunto sintético inspirado nesses dados, contendo o número de gols marcados pela seleção do Brasil por ano de 2000 a 2024. O link para o dataset do Kaggle é:\n",
    "\n",
    "- [International football results from 1872 to 2025 – Kaggle](https://www.kaggle.com/datasets/martj42/international-football-results-from-1872-to-2025)\n",
    "\n",
    "A seguir importamos as bibliotecas necessárias, construímos o dataset sintético e visualizamos a série temporal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b3ea355-065d-49ab-af26-24dfdc752dfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prophet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprophet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Prophet\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sqrt\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'prophet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Criar dataset sintético de gols por ano para a seleção brasileira (2000-2024)\n",
    "years = np.arange(2000, 2025)\n",
    "# Lista de gols anuais (sintético). Os valores foram escolhidos manualmente para criar uma série com variações.\n",
    "goals = [39, 45, 44, 41, 52, 48, 46, 52, 54, 43, 45, 41, 39, 47, 40, 50, 44, 51, 48, 43, 46, 44, 49, 45, 43]\n",
    "\n",
    "# Construir DataFrame\n",
    "futebol_df = pd.DataFrame({'ds': pd.to_datetime(years, format='%Y'), 'y': goals})\n",
    "\n",
    "# Exibir primeiras linhas\n",
    "display(futebol_df.head())\n",
    "\n",
    "# Plotar a série temporal\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(futebol_df['ds'], futebol_df['y'], marker='o')\n",
    "plt.title('Gols anuais da seleção brasileira (sintético)')\n",
    "plt.xlabel('Ano')\n",
    "plt.ylabel('Gols')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Modelo Prophet (ou Sktime)\n",
    "# -----------------------------\n",
    "# Separar em treino e teste (5 últimos anos como teste)\n",
    "train_prophet = futebol_df.iloc[:-5]\n",
    "test_prophet = futebol_df.iloc[-5:]\n",
    "\n",
    "# Instanciar e ajustar o modelo Prophet\n",
    "m = Prophet()\n",
    "m.fit(train_prophet)\n",
    "\n",
    "# Criar DataFrame futuro para previsão nos anos de teste\n",
    "future = m.make_future_dataframe(periods=len(test_prophet), freq='Y')\n",
    "forecast = m.predict(future)\n",
    "\n",
    "# Extrair previsões correspondentes ao período de teste\n",
    "pred_prophet = forecast['yhat'][-len(test_prophet):].values\n",
    "\n",
    "# Calcular RMSE para Prophet\n",
    "rmse_prophet = np.sqrt(mean_squared_error(test_prophet['y'], pred_prophet))\n",
    "print(f\"RMSE do modelo Prophet: {rmse_prophet:.2f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Modelo LSTM\n",
    "# -----------------------------\n",
    "# Escalar os dados entre 0 e 1\n",
    "data_values = futebol_df[['y']].values\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_values = scaler.fit_transform(data_values)\n",
    "\n",
    "# Função para criar sequências\n",
    "def create_sequences(dataset, look_back=3):\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        X.append(dataset[i:(i + look_back), 0])\n",
    "        y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "look_back = 3\n",
    "X_all, y_all = create_sequences(scaled_values, look_back)\n",
    "\n",
    "# Definir tamanho do conjunto de teste (5 últimos anos)\n",
    "train_size = len(futebol_df) - 5 - look_back + 1  # ajustar para sequências\n",
    "X_train, y_train = X_all[:train_size], y_all[:train_size]\n",
    "X_test, y_test = X_all[train_size:], y_all[train_size:]\n",
    "\n",
    "# Ajustar formato para LSTM [samples, timesteps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], look_back, 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], look_back, 1))\n",
    "\n",
    "# Construir modelo LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(look_back, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Treinar o modelo\n",
    "model.fit(X_train, y_train, epochs=200, verbose=0)\n",
    "\n",
    "# Prever\n",
    "y_pred_scaled = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Reverter escala para valores originais\n",
    "y_pred = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_orig = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Calcular RMSE para LSTM\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_test_orig, y_pred))\n",
    "print(f\"RMSE do modelo LSTM: {rmse_lstm:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42860131-c40f-497d-984e-9ee945e419e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f2e2487ddd0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/prophet/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f2e2487c6d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/prophet/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f2e2487e3d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/prophet/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f2e2487ec10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/prophet/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f2e2487f610>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/prophet/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement prophet (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for prophet\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install prophet -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9722bcd3-c387-42ea-916b-9a390ce99752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sktime not available No module named 'sktime'\n",
      "statsmodels available\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import sktime\n",
    "    print('sktime available')\n",
    "except ImportError as e:\n",
    "    print('sktime not available', e)\n",
    "\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    print('statsmodels available')\n",
    "except ImportError as e:\n",
    "    print('statsmodels not available', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10058cc8-edcd-4362-8291-dd81559a1a81",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (208536087.py, line 5)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint('fbprophet not available', e)import importlib.util\u001b[39m\n                                       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import fbprophet\n",
    "    print('fbprophet available')\n",
    "except ImportError as e:\n",
    "    print('fbprophet not available', e)import importlib.util\n",
    "print('sktime spec:', importlib.util.find_spec('sktime'))\n",
    "print('statsmodels spec:', importlib.util.find_spec('statsmodels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa400443-ac42-41aa-b42e-3034ba66bf90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857360cc-1c9d-4c3f-bfa2-7fd17871b35e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c0c1e8-7ed7-4bc9-b9af-60bd0d460428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9672f409-fe8f-49b5-8848-73c4df0adb2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'futebol_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtsa\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mholtwinters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExponentialSmoothing\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Separar em treino e teste\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m train_classic = \u001b[43mfutebol_df\u001b[49m.iloc[:-\u001b[32m5\u001b[39m]\n\u001b[32m      8\u001b[39m test_classic = futebol_df.iloc[-\u001b[32m5\u001b[39m:]\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Ajustar modelo de suavização exponencial com tendência aditiva\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'futebol_df' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Modelo clássico (Exponential Smoothing) usando statsmodels\n",
    "# -----------------------------\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Separar em treino e teste\n",
    "train_classic = futebol_df.iloc[:-5]\n",
    "test_classic = futebol_df.iloc[-5:]\n",
    "\n",
    "# Ajustar modelo de suavização exponencial com tendência aditiva\n",
    "es_model = ExponentialSmoothing(train_classic['y'], trend='add', seasonal=None)\n",
    "es_fit = es_model.fit()\n",
    "pred_es = es_fit.forecast(len(test_classic))\n",
    "\n",
    "# Calcular RMSE\n",
    "rmse_es = np.sqrt(mean_squared_error(test_classic['y'], pred_es))\n",
    "print(f\"RMSE do modelo de Suavizacao Exponencial: {rmse_es:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3533ca71-f594-4687-b398-214bd69e5e05",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sqrt\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Criar dataset sintético\n",
    "years = np.arange(2000, 2025)\n",
    "goals = [39, 45, 44, 41, 52, 48, 46, 52, 54, 43, 45, 41, 39, 47, 40, 50, 44, 51, 48, 43, 46, 44, 49, 45, 43]\n",
    "futebol_df = pd.DataFrame({'ds': pd.to_datetime(years, format='%Y'), 'y': goals})\n",
    "\n",
    "# Visualizar dataset\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(futebol_df['ds'], futebol_df['y'], marker='o')\n",
    "plt.title('Gols anuais da seleção brasileira (sintético)')\n",
    "plt.xlabel('Ano')\n",
    "plt.ylabel('Gols')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --------------------\n",
    "# Modelo clássico: Suavização Exponencial\n",
    "# --------------------\n",
    "train_classic = futebol_df.iloc[:-5]\n",
    "test_classic = futebol_df.iloc[-5:]\n",
    "\n",
    "es_model = ExponentialSmoothing(train_classic['y'], trend='add', seasonal=None)\n",
    "es_fit = es_model.fit()\n",
    "pred_es = es_fit.forecast(len(test_classic))\n",
    "\n",
    "rmse_es = np.sqrt(mean_squared_error(test_classic['y'], pred_es))\n",
    "print(f\"RMSE do modelo de Suavização Exponencial: {rmse_es:.2f}\")\n",
    "\n",
    "# --------------------\n",
    "# Modelo LSTM\n",
    "# --------------------\n",
    "# Escalar dados\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "data_values = futebol_df[['y']].values\n",
    "scaled = scaler.fit_transform(data_values)\n",
    "\n",
    "# Criar sequências\n",
    "def create_sequences(dataset, look_back=3):\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        X.append(dataset[i:(i+look_back), 0])\n",
    "        y.append(dataset[i+look_back, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "look_back = 3\n",
    "X_all, y_all = create_sequences(scaled, look_back)\n",
    "\n",
    "# Train-test split\n",
    "test_size = 5\n",
    "train_size = len(futebol_df) - test_size - look_back\n",
    "X_train = X_all[:train_size]\n",
    "y_train = y_all[:train_size]\n",
    "X_test = X_all[train_size:]\n",
    "y_test = y_all[train_size:]\n",
    "\n",
    "# Reshape para LSTM\n",
    "X_train = X_train.reshape((X_train.shape[0], look_back, 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], look_back, 1))\n",
    "\n",
    "# Construir LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(look_back,1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X_train, y_train, epochs=200, verbose=0)\n",
    "\n",
    "# Prever\n",
    "y_pred_scaled = model.predict(X_test, verbose=0)\n",
    "y_pred = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_orig = scaler.inverse_transform(y_test.reshape(-1,1))\n",
    "\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_test_orig, y_pred))\n",
    "print(f\"RMSE do modelo LSTM: {rmse_lstm:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a3a6183-f8ae-490f-9596-141a506fca0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch not available No module named 'torch'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import torch\n",
    "    print('torch available, version', torch.__version__)\n",
    "except ImportError as e:\n",
    "    print('torch not available', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e40f6c-16d9-417a-a1bf-29ef5d6aaa62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40c90001-b5e5-4799-9fa5-8b3896c7d32b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'goals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 102\u001b[39m\n\u001b[32m    100\u001b[39m look_back = \u001b[32m3\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Usar dados escalados do dataset futebol_df (definido anteriormente)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m values = np.array(\u001b[43mgoals\u001b[49m)  \u001b[38;5;66;03m# usar lista de gols\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# Normalizar valores entre 0 e 1 para estabilidade\u001b[39;00m\n\u001b[32m    104\u001b[39m min_v, max_v = values.min(), values.max()\n",
      "\u001b[31mNameError\u001b[39m: name 'goals' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size=1, hidden_size=10, output_size=1, lr=0.01):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lr = lr\n",
    "        # Weight matrices\n",
    "        self.Wx = np.random.randn(hidden_size, input_size) * 0.1\n",
    "        self.Wh = np.random.randn(hidden_size, hidden_size) * 0.1\n",
    "        self.Wy = np.random.randn(output_size, hidden_size) * 0.1\n",
    "        # Bias vectors\n",
    "        self.b = np.zeros((hidden_size, 1))\n",
    "        self.c = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, x_seq):\n",
    "        '''Forward pass for one sequence. Returns hidden states and output.'''\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        hidden_states = []\n",
    "        for x_t in x_seq:\n",
    "            x_t = x_t.reshape(-1,1)  # ensure column vector\n",
    "            h = np.tanh(self.Wx @ x_t + self.Wh @ h + self.b)\n",
    "            hidden_states.append(h)\n",
    "        y_hat = self.Wy @ h + self.c  # output\n",
    "        return hidden_states, y_hat\n",
    "\n",
    "    def backward(self, x_seq, hidden_states, y_hat, y_true):\n",
    "        '''Backpropagation through time for one sequence.'''\n",
    "        # Initialize gradients\n",
    "        dWx = np.zeros_like(self.Wx)\n",
    "        dWh = np.zeros_like(self.Wh)\n",
    "        dWy = np.zeros_like(self.Wy)\n",
    "        db = np.zeros_like(self.b)\n",
    "        dc = np.zeros_like(self.c)\n",
    "\n",
    "        # Output error\n",
    "        dy = (y_hat - y_true)  # shape (1,1)\n",
    "        dWy += dy @ hidden_states[-1].T  # (1, hidden)\n",
    "        dc += dy\n",
    "\n",
    "        # Backprop through time\n",
    "        dh_next = self.Wy.T @ dy  # (hidden,1)\n",
    "        for t in reversed(range(len(x_seq))):\n",
    "            h_t = hidden_states[t]\n",
    "            # derivative of tanh\n",
    "            dtanh = (1 - h_t * h_t) * dh_next  # (hidden,1)\n",
    "            db += dtanh\n",
    "            x_t = x_seq[t].reshape(-1,1)\n",
    "            dWx += dtanh @ x_t.T\n",
    "            h_prev = hidden_states[t-1] if t>0 else np.zeros_like(h_t)\n",
    "            dWh += dtanh @ h_prev.T\n",
    "            dh_next = self.Wh.T @ dtanh\n",
    "        return dWx, dWh, dWy, db, dc\n",
    "\n",
    "    def update(self, dWx, dWh, dWy, db, dc):\n",
    "        '''Gradient descent update.'''\n",
    "        self.Wx -= self.lr * dWx\n",
    "        self.Wh -= self.lr * dWh\n",
    "        self.Wy -= self.lr * dWy\n",
    "        self.b  -= self.lr * db\n",
    "        self.c  -= self.lr * dc\n",
    "\n",
    "    def train(self, X, y, epochs=100):\n",
    "        # X: list of sequences (n_samples, seq_len), each sequence is array length look_back\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            # accumulate gradients over all sequences (batch gradient descent)\n",
    "            sum_dWx = np.zeros_like(self.Wx)\n",
    "            sum_dWh = np.zeros_like(self.Wh)\n",
    "            sum_dWy = np.zeros_like(self.Wy)\n",
    "            sum_db = np.zeros_like(self.b)\n",
    "            sum_dc = np.zeros_like(self.c)\n",
    "            for seq, target in zip(X, y):\n",
    "                seq = seq.reshape(-1,1)  # shape (seq_len,1)\n",
    "                hidden_states, y_hat = self.forward(seq)\n",
    "                loss = 0.5 * (y_hat.item() - target) ** 2\n",
    "                total_loss += loss\n",
    "                dWx, dWh, dWy, db, dc = self.backward(seq, hidden_states, y_hat, target)\n",
    "                sum_dWx += dWx\n",
    "                sum_dWh += dWh\n",
    "                sum_dWy += dWy\n",
    "                sum_db  += db\n",
    "                sum_dc  += dc\n",
    "            # update weights (use average gradients)\n",
    "            n = len(X)\n",
    "            self.update(sum_dWx / n, sum_dWh / n, sum_dWy / n, sum_db / n, sum_dc / n)\n",
    "            losses.append(total_loss / n)\n",
    "        return losses\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for seq in X:\n",
    "            seq = seq.reshape(-1,1)\n",
    "            _, y_hat = self.forward(seq)\n",
    "            preds.append(y_hat.item())\n",
    "        return np.array(preds)\n",
    "\n",
    "# ----------\n",
    "# Preparar dados para RNN\n",
    "look_back = 3\n",
    "# Usar dados escalados do dataset futebol_df (definido anteriormente)\n",
    "values = np.array(goals)  # usar lista de gols\n",
    "# Normalizar valores entre 0 e 1 para estabilidade\n",
    "min_v, max_v = values.min(), values.max()\n",
    "values_norm = (values - min_v) / (max_v - min_v)\n",
    "\n",
    "# Criar sequências\n",
    "def create_sequences_rnn(data, look_back):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        X.append(data[i:i+look_back])\n",
    "        y.append(data[i+look_back])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_seq, y_seq = create_sequences_rnn(values_norm, look_back)\n",
    "\n",
    "# Dividir treino e teste (5 últimos anos)\n",
    "train_size = len(values_norm) - 5 - look_back\n",
    "X_train_rnn = X_seq[:train_size]\n",
    "y_train_rnn = y_seq[:train_size]\n",
    "X_test_rnn = X_seq[train_size:]\n",
    "y_test_rnn = y_seq[train_size:]\n",
    "\n",
    "# Inicializar e treinar o RNN\n",
    "rnn_model = SimpleRNN(input_size=1, hidden_size=5, output_size=1, lr=0.05)\n",
    "rnn_model.train(X_train_rnn, y_train_rnn, epochs=200)\n",
    "\n",
    "# Prever e reverter normalização\n",
    "pred_norm = rnn_model.predict(X_test_rnn)\n",
    "pred_rnn = pred_norm * (max_v - min_v) + min_v\n",
    "y_test_orig_rnn = y_test_rnn * (max_v - min_v) + min_v\n",
    "\n",
    "rmse_rnn = np.sqrt(mean_squared_error(y_test_orig_rnn, pred_rnn))\n",
    "print(f\"RMSE do modelo RNN (LSTM simplificado): {rmse_rnn:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbb5f926-7135-4ee4-a452-3f226073d75d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_sequences_rnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m values_norm = (data - min_v) / (max_v - min_v)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Criar sequências para RNN\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m X_seq, y_seq = \u001b[43mcreate_sequences_rnn\u001b[49m(values_norm, look_back)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Dividir em treino e teste (5 últimos anos)\u001b[39;00m\n\u001b[32m     15\u001b[39m train_size = \u001b[38;5;28mlen\u001b[39m(data) - \u001b[32m5\u001b[39m - look_back\n",
      "\u001b[31mNameError\u001b[39m: name 'create_sequences_rnn' is not defined"
     ]
    }
   ],
   "source": [
    "# Treinar e avaliar RNN simplificado\n",
    "# Lista de gols sintética\n",
    "goals = [39, 45, 44, 41, 52, 48, 46, 52, 54, 43, 45, 41, 39, 47, 40, 50, 44, 51, 48, 43, 46, 44, 49, 45, 43]\n",
    "look_back = 3\n",
    "\n",
    "# Normalizar valores\n",
    "data = np.array(goals)\n",
    "min_v, max_v = data.min(), data.max()\n",
    "values_norm = (data - min_v) / (max_v - min_v)\n",
    "\n",
    "# Criar sequências para RNN\n",
    "X_seq, y_seq = create_sequences_rnn(values_norm, look_back)\n",
    "\n",
    "# Dividir em treino e teste (5 últimos anos)\n",
    "train_size = len(data) - 5 - look_back\n",
    "X_train_rnn = X_seq[:train_size]\n",
    "y_train_rnn = y_seq[:train_size]\n",
    "X_test_rnn = X_seq[train_size:]\n",
    "y_test_rnn = y_seq[train_size:]\n",
    "\n",
    "# Treinar modelo RNN\n",
    "rnn_model = SimpleRNN(input_size=1, hidden_size=5, output_size=1, lr=0.05)\n",
    "rnn_model.train(X_train_rnn, y_train_rnn, epochs=300)\n",
    "\n",
    "# Previsão\n",
    "y_pred_norm = rnn_model.predict(X_test_rnn)\n",
    "y_pred = y_pred_norm * (max_v - min_v) + min_v\n",
    "\n",
    "# Valores reais para teste (reverter normalização)\n",
    "y_test_orig = y_test_rnn * (max_v - min_v) + min_v\n",
    "\n",
    "rmse_rnn = np.sqrt(mean_squared_error(y_test_orig, y_pred))\n",
    "print(f\"RMSE do modelo RNN (LSTM simplificado): {rmse_rnn:.2f}\")\n",
    "print(\"Previsões RNN:\", y_pred)\n",
    "print(\"Valores reais:\", y_test_orig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e000886-3b9c-4975-83f7-54439a03be7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE do modelo de Suavização Exponencial: 2.31\n",
      "RMSE do modelo RNN (LSTM simplificado): 2.18\n",
      "Previsões RNN: [46.13408418 46.121039   46.13079382 46.10403825 46.12322685]\n",
      "Valores reais: [46. 44. 49. 45. 43.]\n"
     ]
    }
   ],
   "source": [
    "# Calcular modelos e métricas de erro\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Lista de gols (sintética)\n",
    "goals = [39, 45, 44, 41, 52, 48, 46, 52, 54, 43, 45, 41, 39, 47, 40, 50, 44, 51, 48, 43, 46, 44, 49, 45, 43]\n",
    "\n",
    "# 1. Modelo clássico: Suavização exponencial\n",
    "train_es = goals[:-5]\n",
    "test_es = goals[-5:]\n",
    "\n",
    "es_model = ExponentialSmoothing(train_es, trend='add', seasonal=None)\n",
    "es_fit = es_model.fit()\n",
    "pred_es = es_fit.forecast(len(test_es))\n",
    "rmse_es = np.sqrt(mean_squared_error(test_es, pred_es))\n",
    "print(f\"RMSE do modelo de Suavização Exponencial: {rmse_es:.2f}\")\n",
    "\n",
    "# 2. Modelo LSTM simplificado (RNN) implementado com numpy\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size=1, hidden_size=5, output_size=1, lr=0.05):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lr = lr\n",
    "        # Pesos e viéses\n",
    "        self.Wx = np.random.randn(hidden_size, input_size) * 0.1\n",
    "        self.Wh = np.random.randn(hidden_size, hidden_size) * 0.1\n",
    "        self.Wy = np.random.randn(output_size, hidden_size) * 0.1\n",
    "        self.b = np.zeros((hidden_size, 1))\n",
    "        self.c = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, x_seq):\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        hidden_states = []\n",
    "        for x_t in x_seq:\n",
    "            x_t = x_t.reshape(-1,1)\n",
    "            h = np.tanh(self.Wx @ x_t + self.Wh @ h + self.b)\n",
    "            hidden_states.append(h)\n",
    "        y_hat = self.Wy @ h + self.c\n",
    "        return hidden_states, y_hat\n",
    "\n",
    "    def backward(self, x_seq, hidden_states, y_hat, y_true):\n",
    "        dWx = np.zeros_like(self.Wx)\n",
    "        dWh = np.zeros_like(self.Wh)\n",
    "        dWy = np.zeros_like(self.Wy)\n",
    "        db = np.zeros_like(self.b)\n",
    "        dc = np.zeros_like(self.c)\n",
    "\n",
    "        dy = (y_hat - y_true)\n",
    "        dWy += dy @ hidden_states[-1].T\n",
    "        dc += dy\n",
    "\n",
    "        dh_next = self.Wy.T @ dy\n",
    "        for t in reversed(range(len(x_seq))):\n",
    "            h_t = hidden_states[t]\n",
    "            dtanh = (1 - h_t * h_t) * dh_next\n",
    "            db += dtanh\n",
    "            x_t = x_seq[t].reshape(-1,1)\n",
    "            dWx += dtanh @ x_t.T\n",
    "            h_prev = hidden_states[t-1] if t>0 else np.zeros_like(h_t)\n",
    "            dWh += dtanh @ h_prev.T\n",
    "            dh_next = self.Wh.T @ dtanh\n",
    "        return dWx, dWh, dWy, db, dc\n",
    "\n",
    "    def update(self, dWx, dWh, dWy, db, dc):\n",
    "        self.Wx -= self.lr * dWx\n",
    "        self.Wh -= self.lr * dWh\n",
    "        self.Wy -= self.lr * dWy\n",
    "        self.b  -= self.lr * db\n",
    "        self.c  -= self.lr * dc\n",
    "\n",
    "    def train(self, X, y, epochs=300):\n",
    "        for epoch in range(epochs):\n",
    "            # acumular gradientes\n",
    "            sum_dWx = np.zeros_like(self.Wx)\n",
    "            sum_dWh = np.zeros_like(self.Wh)\n",
    "            sum_dWy = np.zeros_like(self.Wy)\n",
    "            sum_db = np.zeros_like(self.b)\n",
    "            sum_dc = np.zeros_like(self.c)\n",
    "            for seq, target in zip(X, y):\n",
    "                seq = seq.reshape(-1,1)\n",
    "                hidden_states, y_hat = self.forward(seq)\n",
    "                dWx, dWh, dWy, db, dc = self.backward(seq, hidden_states, y_hat, target)\n",
    "                sum_dWx += dWx\n",
    "                sum_dWh += dWh\n",
    "                sum_dWy += dWy\n",
    "                sum_db  += db\n",
    "                sum_dc  += dc\n",
    "            n = len(X)\n",
    "            self.update(sum_dWx / n, sum_dWh / n, sum_dWy / n, sum_db / n, sum_dc / n)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for seq in X:\n",
    "            seq = seq.reshape(-1,1)\n",
    "            _, y_hat = self.forward(seq)\n",
    "            preds.append(y_hat.item())\n",
    "        return np.array(preds)\n",
    "\n",
    "# Preparar dados para RNN\n",
    "look_back = 3\n",
    "# Normalizar\n",
    "data_arr = np.array(goals)\n",
    "min_v, max_v = data_arr.min(), data_arr.max()\n",
    "values_norm = (data_arr - min_v) / (max_v - min_v)\n",
    "# Criar sequências\n",
    "X_seq = []\n",
    "y_seq = []\n",
    "for i in range(len(values_norm) - look_back):\n",
    "    X_seq.append(values_norm[i:i+look_back])\n",
    "    y_seq.append(values_norm[i+look_back])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# Dividir treino e teste (5 últimos anos)\n",
    "train_size_rnn = len(goals) - 5 - look_back\n",
    "X_train_rnn = X_seq[:train_size_rnn]\n",
    "y_train_rnn = y_seq[:train_size_rnn]\n",
    "X_test_rnn = X_seq[train_size_rnn:]\n",
    "y_test_rnn = y_seq[train_size_rnn:]\n",
    "\n",
    "# Inicializar e treinar RNN\n",
    "rnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1, lr=0.05)\n",
    "rnn.train(X_train_rnn, y_train_rnn, epochs=300)\n",
    "\n",
    "# Prever e reverter normalização\n",
    "pred_norm = rnn.predict(X_test_rnn)\n",
    "pred_rnn = pred_norm * (max_v - min_v) + min_v\n",
    "y_test_orig_rnn = y_test_rnn * (max_v - min_v) + min_v\n",
    "\n",
    "rmse_rnn = np.sqrt(mean_squared_error(y_test_orig_rnn, pred_rnn))\n",
    "print(f\"RMSE do modelo RNN (LSTM simplificado): {rmse_rnn:.2f}\")\n",
    "print(\"Previsões RNN:\", pred_rnn)\n",
    "print(\"Valores reais:\", y_test_orig_rnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a0f57-6e31-4352-bdf2-def71ef22e87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
